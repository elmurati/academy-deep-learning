LESSON 1 - Examples and Exercises - Textbook http://www.deeplearningbook.org/

1. IMAGE STYLE

$ conda create -n style-transfer python=3			// create environment
$ activate style-transfer					// enter environment
$ conda install tensorflow scipy pillow				// install package
$ pip install moviepy						// install package
$ python -c "import imageio; imageio.plugins.ffmpeg.download()"	// import repo

download the zip file from repo: https://github.com/lengstrom/fast-style-transfer

download the rain-princes.ckpt

$ python evaluate.py --checkpoint ./rain-princess.ckpt --in-path <path_to_input_file> --out-path ./output_image.jpg

----------------------------------------------------------------------------------------

2. FLAPPY BIRD RUN

$ conda create --name=flappybird python=2.7			// create environment
$ source activate flappybird					// enter environment
$ conda install -c menpo opencv3				// install package
$ pip install pygame						// install game
$ pip install tensorflow					// install tensorflow
$ git clone https://github.com/yenchenlin/DeepLearningFlappyBird.git	
$ cd DeepLearningFlappyBird
$ python deep_q_network.py

----------------------------------------------------------------------------------------

3. JUPYTER NOTEBOOKS


$ pip install jupyter notebook					// install notebook
open browser to http://localhost:8888				// access notebook

----------------------------------------------------------------------------------------

4. SOFTMAX FUNCTION


We use softmax function to classify data.

	- We get the Linear Function Scores Z1, ...., Zn
	- For each class i we calculate probability as P(i) = e^Zi/(e^Z1 + ... + e^Zn)

----------------------------------------------------------------------------------------

5. CROSS ENTROPY


For each combination we calculate P(x) and P(~x). We multiply the probabilities for each 
combination. so given three events, 1, 2, and 3. We take all the possible combinations
and multiply them, for example:
	
	P(1) * P(2) * (P~3) = P(s1)
	P(1) * P(~2) * (P3) = P(s2)
	P(1) * P(~2) * (P~3) = P(s3) ...

	We know that P(s1) + ... + P(sn) = P(S) = 1
	Then we consider the negative logarithms of these probabilities

	P(s1) --> -ln(P(s1) this is the ENTROPY. Low probability relates to high ENTROPY.

----------------------------------------------------------------------------------------

6. GRADIENT DESCENT

We take the sigmoid function and calculate the derivative:  σ′(x)=σ(x)(1−σ(x))

----------------------------------------------------------------------------------------

7. ERRORS


	* Under-fitting = error due to bias

	* Over-fitting

To solve this issue we train the model over an increasing amount of data. For each data
set we measure the Training Error and the Testing Error. The training error will always
go down wheres the testing error will reach a global minimum. MODEL-COMPLEXITY GRAPH.

Use gradient descent to find the minimum point --> EARLY STOPPING ALGORITHM

LARGE COEFFICIENTS --> OVERFITTING

REGULARIZATION - Penalize big wieghts

	* L1 Reg: lambda(|w1| + ... + |wn|) --> good for feature selection
	* L2 Reg: lambda(w1^2 + ... + wn^2) --> good for training models

----------------------------------------------------------------------------------------

8. LEARNING RATE

	* HIGH LEARNING RATE - overshooting
	* LOW LEARNING RATE - slow processing

----------------------------------------------------------------------------------------


	


